<div align="center">
<img src="docs/static/images/sandra_logo_cut.png" width="220">
<h1>SanDRA</h1>
<h3>Safe LLM-based Decision-making for Automated Vehicles</h3>
 </div>

![SanDRA GIF](docs/static/videos/sandra_animation.gif)
## ‚öôÔ∏è Setup
For using **SanDRA** with OpenAI models, you need an OpenAI API-key. Make sure to export it as environment variable named **OPENAI_API_KEY**.
If you'd rather use local models, you can follow the instructions in section **Run with local LLMs**.

## üì¶ Dependencies for Reachability Analysis  
For leveraging reachability analysis you need to install
* [commonroad-reach-semantic](https://github.com/CommonRoad/commonroad-reach-semantic/tree/feature/sandra): branch `feature/sandra` 
(use `export CXX=/usr/bin/g++-10` before installation to use the correct compiler, the whole installation process might take **more than 10 minutes**.)
> **Note:** After installation, please go to `~/SanDRA/sandra/config.py` and update `COMMONROAD_REACH_SEMANTIC_ROOT` to the directory where you installed `commonroad-reach-semantic`.

##  üì¶ Dependencies for Set-based Predictions
For set-based predictions, you need to install
* [sonia (spot)](https://github.com/CommonRoad/spot-sonia): branch `master` (`python setup.py install`)

## üîÑ Roadmap  

- [x] üìÑ Release Paper  
- [x] üì¶ Release Code
- [x] üåê Release Project Page
- [ ] üìë Release Updated Paper  


## ‚ñ∂Ô∏è Main scripts
There are 2 ways to test SanDRA:
1. With a [CommonRoad](https://commonroad.in.tum.de/) scenario.
2. With the [highwayenv](https://highway-env.farama.org/).

*commonroad_run.py* and  *highwayenv_run.py* illustrate how to run SanDRA decision making in either of these cases. Please make sure to prepare the seeds for highwayenv / the scenarios for CommonRoad beforehand.

## üñ•Ô∏è Run with local LLMs
To run SanDRA with local models, you need to follow these steps:
1. üì• Download [Ollama](https://ollama.com/download)
2. ‚öôÔ∏è Install Go (Recommended):
```bash
sudo apt update
sudo apt install golang-go
```
3. ü§ñ Download a model (We recommend to use a model with >=8B parameters to avoid problems with structured outputs):
```bash
ollama pull qwen3:8b
```
4. ‚ñ∂Ô∏è Start the Ollama server
```bash
ollama serve
```
## üìù Cite Us  

If you use **SanDRA** in your research, please cite:  

```bibtex
@article{lin2025sandra,
  title     = {SanDRA: Safe Large-Language-Model-Based Decision Making for Automated Vehicles Using Reachability Analysis},
  author    = {Yuanfei Lin and Sebastian Illing and Matthias Althoff},
  journal   = {arXiv preprint arXiv:2501.xxxxx}, 
  year      = {2025}
}